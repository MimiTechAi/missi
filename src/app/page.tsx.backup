"use client";

import { useState, useRef, useCallback, useEffect, useMemo } from "react";
import VoiceOrb from "@/components/VoiceOrb";

// ============================================================
// Types
// ============================================================
type ToolResult = {
  tool: string;
  args: Record<string, string>;
  result: string;
  duration: number;
};

type Document = {
  title: string;
  content: string;
  type: string;
};

type ModelRoute = {
  model: string;
  label: string;
  reason: string;
};

type Message = {
  role: "user" | "assistant";
  content: string;
  toolCalls?: ToolResult[];
  model?: ModelRoute;
  plan?: string[];
  documents?: Document[];
  image?: string;
  timestamp?: number;
  displayedContent?: string;
  responseTime?: number; // ms from send to response
  fromVoice?: boolean; // true if input came from STT
};

type VoiceState = "idle" | "listening" | "thinking" | "speaking";

// ============================================================
// Persistence
// ============================================================
const STORAGE_KEY = "missi-history-v2";
function loadMessages(): Message[] {
  if (typeof window === "undefined") return [];
  try {
    const saved = localStorage.getItem(STORAGE_KEY);
    if (!saved) return [];
    // Strip displayedContent on load â€” it's a transient animation field
    return JSON.parse(saved).map((m: Message) => {
      const { displayedContent, ...rest } = m;
      return rest;
    });
  } catch { return []; }
}
function saveMessages(msgs: Message[]) {
  // Don't persist displayedContent â€” it's transient
  try { localStorage.setItem(STORAGE_KEY, JSON.stringify(msgs.slice(-50).map(m => {
    const { displayedContent, ...rest } = m;
    return rest;
  }))); } catch {}
}

// ============================================================
// Typing Hook
// ============================================================
function useTypingEffect(text: string, speed: number = 18, enabled: boolean = true): string {
  const [displayed, setDisplayed] = useState("");
  const indexRef = useRef(0);

  useEffect(() => {
    if (!enabled) { setDisplayed(text); return; }
    setDisplayed("");
    indexRef.current = 0;

    const interval = setInterval(() => {
      indexRef.current += 1;
      // Type faster â€” 3 chars at a time for natural feel
      const chars = Math.min(3, text.length - indexRef.current);
      indexRef.current += chars;
      
      if (indexRef.current >= text.length) {
        setDisplayed(text);
        clearInterval(interval);
      } else {
        setDisplayed(text.slice(0, indexRef.current));
      }
    }, speed);

    return () => clearInterval(interval);
  }, [text, speed, enabled]);

  return displayed;
}

// ============================================================
// Main Component
// ============================================================
export default function Home() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState("");
  const [voiceState, setVoiceState] = useState<VoiceState>("idle");
  const [isLoading, setIsLoading] = useState(false);
  const [audioLevel, setAudioLevel] = useState(0);
  const [continuousMode, setContinuousMode] = useState(false);
  const [showChat, setShowChat] = useState(true);
  const [currentModel, setCurrentModel] = useState<ModelRoute | null>(null);
  const [currentPlan, setCurrentPlan] = useState<string[] | null>(null);
  const [dragOver, setDragOver] = useState(false);
  const [latestContent, setLatestContent] = useState("");
  const [spokenSoFar, setSpokenSoFar] = useState(""); // Syncs with TTS playback
  const [thinkingStatus, setThinkingStatus] = useState(""); // Live tool progress
  const [activeTools, setActiveTools] = useState<{ tool: string; args: Record<string, string>; status: "running" | "done"; result?: string; duration?: number }[]>([]); // Live streaming tools
  // Permission-gated services
  const [permissions, setPermissions] = useState<{
    gmailToken: string | null;
    folderFiles: string[] | null;
  }>({ gmailToken: null, folderFiles: null });
  const [sttLang, setSttLang] = useState("en-US");
  
  // Auto-detect browser language on mount
  useEffect(() => {
    const browserLang = navigator.language || "en-US";
    // Map common browser langs to STT-compatible codes
    const langMap: Record<string, string> = {
      "de": "de-DE", "de-DE": "de-DE", "de-AT": "de-AT", "de-CH": "de-CH",
      "fr": "fr-FR", "fr-FR": "fr-FR",
      "es": "es-ES", "es-ES": "es-ES",
      "ja": "ja-JP", "ja-JP": "ja-JP",
      "zh": "zh-CN", "zh-CN": "zh-CN",
      "ko": "ko-KR", "ko-KR": "ko-KR",
      "pt": "pt-BR", "pt-BR": "pt-BR",
      "it": "it-IT", "it-IT": "it-IT",
      "en": "en-US", "en-US": "en-US", "en-GB": "en-GB",
    };
    const detected = langMap[browserLang] || langMap[browserLang.split("-")[0]] || "en-US";
    setSttLang(detected);
  }, []);
  
  // Keep ref in sync for callbacks
  useEffect(() => { sttLangRef.current = sttLang; }, [sttLang]);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const audioRef = useRef<HTMLAudioElement>(null);
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const recognitionRef = useRef<any>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const audioLevelAnimRef = useRef<number>(0);
  const shouldRelistenRef = useRef(false);
  const connectedRef = useRef(false);
  const fileInputRef = useRef<HTMLInputElement>(null);
  const inputRef = useRef<HTMLInputElement>(null);
  const sttLangRef = useRef("en-US");
  const sendMessageRef = useRef<(text: string, image?: string, fromVoice?: boolean) => Promise<void>>(undefined);
  const startListeningRef = useRef<() => void>(undefined);

  // Typing animation for latest assistant message (orb view)
  const typedContent = useTypingEffect(latestContent, 12, latestContent.length > 0);

  // Load messages on mount
  useEffect(() => { setMessages(loadMessages()); }, []);
  useEffect(() => { if (messages.length > 0) saveMessages(messages); }, [messages]);
  useEffect(() => { messagesEndRef.current?.scrollIntoView({ behavior: "smooth" }); }, [messages]);

  // Keyboard shortcuts
  useEffect(() => {
    const handler = (e: KeyboardEvent) => {
      // Space to start/stop listening (when not typing)
      if (e.code === "Space" && document.activeElement?.tagName !== "INPUT" && document.activeElement?.tagName !== "TEXTAREA") {
        e.preventDefault();
        if (voiceState === "idle" && !continuousMode) activate();
        else if (voiceState === "idle" && continuousMode) startListeningRef.current?.();
        else if (voiceState === "listening") {
          recognitionRef.current?.stop();
        }
      }
      // Escape to interrupt / deactivate
      if (e.code === "Escape") {
        deactivate();
      }
      // Cmd/Ctrl+K to focus input
      if ((e.metaKey || e.ctrlKey) && e.key === "k") {
        e.preventDefault();
        inputRef.current?.focus();
      }
    };
    window.addEventListener("keydown", handler);
    return () => window.removeEventListener("keydown", handler);
  // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [voiceState]);

  // Audio level monitoring
  useEffect(() => {
    const audio = audioRef.current;
    if (!audio) return;
    const handlePlay = () => {
      if (connectedRef.current) {
        const analyser = analyserRef.current;
        if (analyser) {
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          const tick = () => {
            if (audio.paused) { setAudioLevel(0); return; }
            analyser.getByteFrequencyData(dataArray);
            setAudioLevel((dataArray.reduce((a, b) => a + b, 0) / dataArray.length / 255) * 2.5);
            audioLevelAnimRef.current = requestAnimationFrame(tick);
          };
          tick();
        }
        return;
      }
      connectedRef.current = true;
      if (!audioContextRef.current) audioContextRef.current = new AudioContext();
      const ctx = audioContextRef.current;
      const analyser = ctx.createAnalyser();
      analyser.fftSize = 256;
      const source = ctx.createMediaElementSource(audio);
      source.connect(analyser);
      analyser.connect(ctx.destination);
      analyserRef.current = analyser;
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      const tick = () => {
        if (audio.paused) { setAudioLevel(0); return; }
        analyser.getByteFrequencyData(dataArray);
        setAudioLevel((dataArray.reduce((a, b) => a + b, 0) / dataArray.length / 255) * 2.5);
        audioLevelAnimRef.current = requestAnimationFrame(tick);
      };
      tick();
    };
    audio.addEventListener("play", handlePlay);
    audio.addEventListener("ended", () => setAudioLevel(0));
    return () => audio.removeEventListener("play", handlePlay);
  }, []);

  // TTS â€” synchronized text + speech display
  const speakText = useCallback(async (text: string, messageIndex?: number) => {
    const sentences = text.match(/[^.!?]+[.!?]+|[^.!?]+$/g) || [text];
    
    // Group sentences in pairs for natural speech flow
    const chunks: string[] = [];
    for (let i = 0; i < sentences.length; i += 2) {
      const group = sentences.slice(i, i + 2).map(s => s.trim()).filter(s => s.length > 1).join(" ");
      if (group) chunks.push(group);
    }
    
    setVoiceState("speaking");
    setSpokenSoFar(""); // Reset
    let accumulated = "";
    
    for (const chunk of chunks) {
      // Reveal this chunk's text BEFORE audio plays (like subtitles appearing)
      accumulated += (accumulated ? " " : "") + chunk;
      setSpokenSoFar(accumulated);
      
      // Update the message in-place so chat panel shows progressive text
      if (messageIndex !== undefined) {
        setMessages(prev => {
          const updated = [...prev];
          if (updated[messageIndex]) {
            updated[messageIndex] = { ...updated[messageIndex], displayedContent: accumulated };
          }
          return updated;
        });
      }
      
      try {
        const res = await fetch("/api/tts", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text: chunk, language: sttLangRef.current, voiceId: currentVoiceId }),
        });
        if (res.ok) {
          const blob = await res.blob();
          const url = URL.createObjectURL(blob);
          await new Promise<void>((resolve) => {
            if (audioRef.current) {
              audioRef.current.src = url;
              audioRef.current.onended = () => { URL.revokeObjectURL(url); resolve(); };
              audioRef.current.onerror = () => { URL.revokeObjectURL(url); resolve(); };
              audioRef.current.play().catch(() => resolve());
            } else resolve();
          });
        } else {
          // ElevenLabs failed (quota, auth, etc) â€” use browser TTS fallback
          await new Promise<void>((resolve) => {
            const u = new SpeechSynthesisUtterance(chunk);
            u.lang = sttLangRef.current;
            u.onend = () => resolve();
            u.onerror = () => resolve();
            speechSynthesis.speak(u);
          });
        }
      } catch {
        // Network error â€” use browser TTS fallback
        await new Promise<void>((resolve) => {
          const u = new SpeechSynthesisUtterance(chunk);
          u.lang = sttLangRef.current;
          u.onend = () => resolve();
          u.onerror = () => resolve();
          speechSynthesis.speak(u);
        });
      }
    }
    
    // Reveal full text at the end
    setSpokenSoFar(text);
    if (messageIndex !== undefined) {
      setMessages(prev => {
        const updated = [...prev];
        if (updated[messageIndex]) {
          updated[messageIndex] = { ...updated[messageIndex], displayedContent: text };
        }
        return updated;
      });
    }
    
    setVoiceState("idle");
    setAudioLevel(0);
    if (shouldRelistenRef.current && startListeningRef.current) {
      setTimeout(() => startListeningRef.current?.(), 400);
    }
  }, []); // No deps needed â€” uses refs

  // Image to base64
  const imageToBase64 = (file: File): Promise<string> =>
    new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => resolve(reader.result as string);
      reader.onerror = reject;
      reader.readAsDataURL(file);
    });

  // Send message
  const sendMessage = useCallback(async (text: string, imageData?: string, fromVoice?: boolean) => {
    if (!text.trim() && !imageData) return;
    const userMessage: Message = { role: "user", content: text, image: imageData, timestamp: Date.now(), fromVoice: !!fromVoice };
    setMessages((prev) => [...prev, userMessage]);
    setInput("");
    setIsLoading(true);
    setVoiceState("thinking");
    setShowChat(true);
    setCurrentModel(null);
    setCurrentPlan(null);
    setLatestContent("");
    setThinkingStatus("");

    // Filler audio â€” speak a brief acknowledgment while LLM is thinking
    const fillers: Record<string, string[]> = {
      "de": ["Moment...", "Einen Augenblick...", "Lass mich nachsehen..."],
      "fr": ["Un instant...", "Laissez-moi vÃ©rifier..."],
      "es": ["Un momento...", "DÃ©jame revisar..."],
      "en": ["Let me check...", "One moment...", "Looking into that..."],
    };
    const langKey = sttLangRef.current.split("-")[0];
    const fillerList = fillers[langKey] || fillers["en"];
    const filler = fillerList[Math.floor(Math.random() * fillerList.length)];
    
    // Fire filler TTS (don't await â€” let it play while API call runs)
    const fillerPromise = (async () => {
      try {
        const res = await fetch("/api/tts", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text: filler, language: sttLangRef.current, voiceId: currentVoiceIdRef.current }),
        });
        if (res.ok && audioRef.current) {
          const blob = await res.blob();
          const url = URL.createObjectURL(blob);
          audioRef.current.src = url;
          audioRef.current.onended = () => URL.revokeObjectURL(url);
          await audioRef.current.play().catch(() => {});
        } else if (!res.ok) {
          // ElevenLabs quota/error â€” browser TTS fallback
          const u = new SpeechSynthesisUtterance(filler);
          u.lang = sttLangRef.current;
          speechSynthesis.speak(u);
        }
      } catch {}
    })();

    try {
      const chatMessages = [...messages, userMessage].map((m) => ({ role: m.role, content: m.content }));
      
      // API call runs IN PARALLEL with filler audio
      const apiStart = Date.now();
      const res = await fetch("/api/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          messages: chatMessages,
          image: imageData || undefined,
          permissions: {
            gmailToken: permissions.gmailToken || undefined,
            fileIndex: permissions.folderFiles?.join("\n") || undefined,
            location: userLocation || undefined,
          },
        }),
      });

      // â”€â”€ SSE Stream Reader â”€â”€
      const reader = res.body!.getReader();
      const decoder = new TextDecoder();
      let buffer = "";
      let streamedContent = "";
      let streamedModel: ModelRoute | null = null;
      let streamedPlan: string[] | null = null;
      let streamedToolResults: ToolResult[] = [];
      let streamedDocuments: Document[] = [];
      const activeTools: { tool: string; args: Record<string, string>; status: "running" | "done"; result?: string; duration?: number }[] = [];

      // Live tool cards state
      setActiveTools([]);

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const events = buffer.split("\n\n");
        buffer = events.pop() || "";

        for (const eventBlock of events) {
          const eventMatch = eventBlock.match(/^event: (.+)$/m);
          const dataMatch = eventBlock.match(/^data: (.+)$/m);
          if (!eventMatch || !dataMatch) continue;

          const eventType = eventMatch[1];
          let eventData;
          try { eventData = JSON.parse(dataMatch[1]); } catch { continue; }

          switch (eventType) {
            case "model_selected":
              streamedModel = eventData;
              setCurrentModel(eventData);
              break;

            case "plan":
              streamedPlan = eventData;
              setCurrentPlan(eventData);
              break;

            case "status":
              setThinkingStatus(String(eventData));
              break;

            case "tool_start": {
              const toolEntry = { tool: eventData.tool, args: eventData.args, status: "running" as const };
              activeTools.push(toolEntry);
              setActiveTools([...activeTools]);
              // Live thinking status
              const toolStatusIcons: Record<string, string> = {
                web_search: "ğŸ” Searching the web", get_weather: "ğŸŒ¤ï¸ Checking weather",
                get_time: "ğŸ• Getting time", calculate: "ğŸ”¢ Calculating",
                run_code: "ğŸ’» Running code", read_webpage: "ğŸ“„ Reading webpage",
                create_document: "ğŸ“ Creating document", translate: "ğŸŒ Translating",
                analyze_data: "ğŸ“Š Analyzing data", generate_code: "âŒ¨ï¸ Generating code",
                set_reminder: "â° Setting reminder", summarize_text: "ğŸ“‹ Summarizing",
                search_gmail: "ğŸ“§ Searching Gmail", read_gmail: "ğŸ“§ Reading email", search_files: "ğŸ“‚ Searching files",
                get_calendar: "ğŸ“… Checking calendar", get_stock_price: "ğŸ“ˆ Fetching stock price",
                get_crypto_price: "ğŸª™ Fetching crypto price", wikipedia: "ğŸ“– Searching Wikipedia",
                get_location: "ğŸ“ Getting location", change_voice: "ğŸ­ Changing voice",
              };
              const argPreview = Object.values(eventData.args)[0] || "";
              setThinkingStatus(`${toolStatusIcons[eventData.tool] || eventData.tool}${argPreview ? `: "${argPreview}"` : ""}...`);
              break;
            }

            case "tool_result": {
              // Mark tool as done
              const idx = activeTools.findIndex(t => t.tool === eventData.tool && t.status === "running");
              if (idx >= 0) {
                activeTools[idx] = { ...activeTools[idx], status: "done", result: eventData.result, duration: eventData.duration };
                setActiveTools([...activeTools]);
              }
              streamedToolResults.push(eventData);
              // Handle special tool results
              if (eventData.tool === "change_voice") {
                try {
                  const voiceData = JSON.parse(eventData.result);
                  if (voiceData._type === "voice_change") {
                    setCurrentVoiceId(voiceData.voiceId);
                  }
                } catch {}
              }
              if (eventData.tool === "set_reminder") {
                try {
                  const reminderData = JSON.parse(eventData.result);
                  if (reminderData._type === "reminder") {
                    scheduleReminder(reminderData.message, 60000); // Default 1 min
                  }
                } catch {}
              }
              playSound("success");
              break;
            }

            case "content":
              streamedContent = String(eventData);
              setLatestContent(streamedContent);
              break;

            case "done":
              if (eventData.documents) streamedDocuments = eventData.documents;
              if (eventData.model) streamedModel = eventData.model;
              break;

            case "error":
              setMessages(prev => [...prev, { role: "assistant", content: `Error: ${eventData.message}`, timestamp: Date.now() }]);
              setVoiceState("idle");
              setIsLoading(false);
              return;
          }
        }
      }

      // Stop filler audio
      await fillerPromise;
      if (audioRef.current && !audioRef.current.paused) {
        audioRef.current.pause();
        audioRef.current.currentTime = 0;
      }

      if (!streamedContent) {
        setVoiceState("idle");
      } else {
        const responseTime = Date.now() - apiStart;
        const newMsg: Message = {
          role: "assistant",
          content: streamedContent,
          displayedContent: "",
          toolCalls: streamedToolResults,
          model: streamedModel || undefined,
          plan: streamedPlan || undefined,
          documents: streamedDocuments,
          timestamp: Date.now(),
          responseTime,
        };

        let msgIndex = 0;
        setMessages(prev => {
          msgIndex = prev.length;
          return [...prev, newMsg];
        });

        await speakText(streamedContent, msgIndex);
      }
      setActiveTools([]);
    } catch {
      setMessages((prev) => [...prev, { role: "assistant", content: "Connection lost. Please try again.", timestamp: Date.now() }]);
      setVoiceState("idle");
    } finally { setIsLoading(false); }
  }, [messages, speakText]);

  // Keep ref in sync
  useEffect(() => { sendMessageRef.current = sendMessage; }, [sendMessage]);

  // STT with silence detection and auto-relisten
  const startListening = useCallback(() => {
    // Interrupt any playing audio
    if (audioRef.current && !audioRef.current.paused) {
      audioRef.current.pause();
      audioRef.current.currentTime = 0;
      speechSynthesis.cancel();
      setAudioLevel(0);
    }
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const SR = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    if (!SR) { alert("Speech recognition requires Chrome."); return; }
    
    const recognition = new SR();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = sttLangRef.current;
    
    let silenceTimer: ReturnType<typeof setTimeout> | null = null;
    let finalTranscript = "";
    let hasSpeech = false;
    
    recognition.onstart = () => {
      setVoiceState("listening");
      setInput("");
      finalTranscript = "";
      hasSpeech = false;
    };
    
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    recognition.onresult = (event: any) => {
      hasSpeech = true;
      let interim = "";
      finalTranscript = "";
      
      for (let i = 0; i < event.results.length; i++) {
        const result = event.results[i];
        if (result.isFinal) {
          finalTranscript += result[0].transcript;
        } else {
          interim += result[0].transcript;
        }
      }
      
      setInput(finalTranscript + interim);
      
      if (silenceTimer) clearTimeout(silenceTimer);
      
      if (finalTranscript.trim()) {
        silenceTimer = setTimeout(() => {
          recognition.stop();
          // Use ref to get latest sendMessage
          sendMessageRef.current?.(finalTranscript.trim(), undefined, true);
        }, 1000); // 1s silence = done (VITA-Audio paper: <500ms ideal)
      }
    };
    
    recognition.onerror = (e: { error: string }) => {
      if (e.error === "no-speech") {
        if (shouldRelistenRef.current) {
          setTimeout(() => startListeningRef.current?.(), 500);
        } else {
          setVoiceState("idle");
        }
      } else if (e.error === "aborted") {
        // Intentional â€” do nothing
      } else {
        setVoiceState("idle");
      }
    };
    
    recognition.onend = () => {
      if (silenceTimer) clearTimeout(silenceTimer);
    };
    
    recognitionRef.current = recognition;
    recognition.start();
  }, []); // No deps â€” uses refs for everything

  // Keep ref in sync
  useEffect(() => { startListeningRef.current = startListening; }, [startListening]);

  // Activate = start continuous conversation with greeting
  const activate = useCallback(() => {
    setContinuousMode(true);
    shouldRelistenRef.current = true;
    
    // Welcome greeting based on detected language
    const greetings: Record<string, string> = {
      "de-DE": "Hallo! Ich bin Missi. Wie kann ich dir helfen?",
      "de-AT": "Hallo! Ich bin Missi. Wie kann ich dir helfen?",
      "de-CH": "Hallo! Ich bin Missi. Wie kann ich dir helfen?",
      "fr-FR": "Bonjour! Je suis Missi. Comment puis-je vous aider?",
      "es-ES": "Â¡Hola! Soy Missi. Â¿En quÃ© puedo ayudarte?",
      "ja-JP": "ã“ã‚“ã«ã¡ã¯ï¼ãƒŸãƒƒã‚·ãƒ¼ã§ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ",
      "zh-CN": "ä½ å¥½ï¼æˆ‘æ˜¯Missiã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ",
      "en-US": "Hello! I'm Missi, your AI assistant. How can I help you?",
      "en-GB": "Hello! I'm Missi, your AI assistant. How can I help you?",
    };
    const greeting = greetings[sttLangRef.current] || greetings["en-US"];
    
    // Speak greeting, then start listening
    (async () => {
      setVoiceState("speaking");
      try {
        const res = await fetch("/api/tts", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text: greeting, language: sttLangRef.current, voiceId: currentVoiceIdRef.current }),
        });
        if (res.ok) {
          const blob = await res.blob();
          const url = URL.createObjectURL(blob);
          await new Promise<void>((resolve) => {
            if (audioRef.current) {
              audioRef.current.src = url;
              audioRef.current.onended = () => { URL.revokeObjectURL(url); resolve(); };
              audioRef.current.onerror = () => { URL.revokeObjectURL(url); resolve(); };
              audioRef.current.play().catch(() => resolve());
            } else resolve();
          });
        } else {
          // ElevenLabs unavailable â€” browser TTS fallback for greeting
          await new Promise<void>((resolve) => {
            const u = new SpeechSynthesisUtterance(greeting);
            u.lang = sttLangRef.current;
            u.onend = () => resolve();
            u.onerror = () => resolve();
            speechSynthesis.speak(u);
          });
        }
      } catch {
        // Network error â€” browser TTS fallback
        await new Promise<void>((resolve) => {
          const u = new SpeechSynthesisUtterance(greeting);
          u.lang = sttLangRef.current;
          u.onend = () => resolve();
          u.onerror = () => resolve();
          speechSynthesis.speak(u);
        });
      }
      setVoiceState("idle");
      setAudioLevel(0);
      startListening();
    })();
  }, [startListening]);

  // Deactivate = stop everything
  const deactivate = useCallback(() => {
    setContinuousMode(false);
    shouldRelistenRef.current = false;
    recognitionRef.current?.stop();
    if (audioRef.current) { audioRef.current.pause(); audioRef.current.currentTime = 0; }
    speechSynthesis.cancel();
    setVoiceState("idle");
    setAudioLevel(0);
    setInput("");
  }, []);

  const handleOrbClick = useCallback(() => {
    if (voiceState === "idle" && !continuousMode) {
      activate();
    } else if (voiceState === "idle" && continuousMode) {
      startListening();
    } else if (voiceState === "listening") {
      if (input.trim()) {
        recognitionRef.current?.stop();
        sendMessageRef.current?.(input.trim(), undefined, true);
      } else {
        deactivate();
      }
    } else if (voiceState === "speaking") {
      if (audioRef.current) { audioRef.current.pause(); audioRef.current.currentTime = 0; }
      speechSynthesis.cancel();
      setAudioLevel(0);
      startListening();
    }
  }, [voiceState, continuousMode, input, activate, deactivate, startListening]);

  const handleImageUpload = async (file: File) => {
    if (!file.type.startsWith("image/")) return;
    sendMessage(input || "Analyze this image", await imageToBase64(file));
  };

  const handleKeyDown = (e: React.KeyboardEvent) => {
    if (e.key === "Enter" && !e.shiftKey) { e.preventDefault(); sendMessage(input); }
  };

  const downloadDocument = (doc: Document) => {
    const blob = new Blob([`# ${doc.title}\n\n${doc.content}`], { type: "text/markdown" });
    const url = URL.createObjectURL(blob);
    const a = document.createElement("a");
    a.href = url; a.download = `${doc.title.replace(/\s+/g, "-").toLowerCase()}.md`;
    a.click(); URL.revokeObjectURL(url);
  };

  const clearConversation = () => {
    setMessages([]); localStorage.removeItem(STORAGE_KEY);
    setShowChat(false); setCurrentModel(null); setCurrentPlan(null); setLatestContent("");
  };

  // â”€â”€ Permission: Connect Folder (File System Access API) â”€â”€
  const connectFolder = useCallback(async () => {
    try {
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      const dirHandle = await (window as any).showDirectoryPicker({ mode: "read" });
      const fileList: string[] = [];
      async function scanDir(handle: FileSystemDirectoryHandle, path: string) {
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        for await (const entry of (handle as any).values()) {
          if (entry.kind === "file") {
            fileList.push(`${path}/${entry.name}`);
          } else if (entry.kind === "directory" && !entry.name.startsWith(".")) {
            await scanDir(entry, `${path}/${entry.name}`);
          }
        }
      }
      await scanDir(dirHandle, dirHandle.name);
      setPermissions(prev => ({ ...prev, folderFiles: fileList }));
    } catch {
      // User cancelled
    }
  }, []);

  // â”€â”€ Permission: Connect Gmail (OAuth2 popup) â”€â”€
  const connectGmail = useCallback(() => {
    const popup = window.open("/api/auth/gmail", "gmail_auth", "width=500,height=600");
    const handler = async (event: MessageEvent) => {
      if (event.data?.type === "gmail_auth" && event.data.code) {
        window.removeEventListener("message", handler);
        popup?.close();
        // Exchange code for token
        try {
          const res = await fetch("/api/auth/gmail", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ code: event.data.code }),
          });
          const data = await res.json();
          if (data.access_token) {
            setPermissions(prev => ({ ...prev, gmailToken: data.access_token }));
          }
        } catch {}
      }
    };
    window.addEventListener("message", handler);
  }, []);

  // â”€â”€ Sound Effects (Web Audio API) â”€â”€
  const playSound = useCallback((type: "activate" | "success" | "error") => {
    try {
      const ctx = new AudioContext();
      const osc = ctx.createOscillator();
      const gain = ctx.createGain();
      osc.connect(gain);
      gain.connect(ctx.destination);
      gain.gain.value = 0.08;
      if (type === "activate") { osc.frequency.value = 880; gain.gain.value = 0.06; }
      else if (type === "success") { osc.frequency.value = 660; }
      else { osc.frequency.value = 220; }
      osc.start();
      osc.stop(ctx.currentTime + 0.12);
    } catch {}
  }, []);

  // â”€â”€ Geolocation â”€â”€
  const [userLocation, setUserLocation] = useState<string>("");
  useEffect(() => {
    if (navigator.geolocation) {
      navigator.geolocation.getCurrentPosition(
        async (pos) => {
          const { latitude, longitude } = pos.coords;
          try {
            const res = await fetch(`https://geocoding-api.open-meteo.com/v1/search?name=&count=1&latitude=${latitude}&longitude=${longitude}`);
            const locStr = `Latitude: ${latitude.toFixed(4)}, Longitude: ${longitude.toFixed(4)}`;
            setUserLocation(locStr);
            // Reverse geocode not available via open-meteo, but coords are enough
          } catch {
            setUserLocation(`Lat ${latitude.toFixed(4)}, Lon ${longitude.toFixed(4)}`);
          }
        },
        () => {} // permission denied â€” that's ok
      );
    }
  }, []);

  // â”€â”€ Wake Word Detection ("Hey Missi") â”€â”€
  const activateRef = useRef(activate);
  useEffect(() => { activateRef.current = activate; }, [activate]);
  
  useEffect(() => {
    if (continuousMode || voiceState !== "idle") return; // Already active
    if (typeof window === "undefined" || !("webkitSpeechRecognition" in window || "SpeechRecognition" in window)) return;
    
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    const wakeRecog = new SpeechRecognition();
    wakeRecog.continuous = true;
    wakeRecog.interimResults = true;
    wakeRecog.lang = sttLangRef.current;

    wakeRecog.onresult = (event: { results: { transcript: string }[][] }) => {
      for (let i = 0; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript.toLowerCase();
        if (transcript.includes("hey missi") || transcript.includes("missi") || transcript.includes("hey missy") || transcript.includes("missi")) {
          wakeRecog.stop();
          playSound("activate");
          // Call activate() which speaks greeting + starts listening
          activateRef.current();
          return;
        }
      }
    };
    wakeRecog.onend = () => {
      // Restart wake word listener if still idle
      if (!continuousMode) {
        try { wakeRecog.start(); } catch {}
      }
    };
    try { wakeRecog.start(); } catch {}
    return () => { try { wakeRecog.stop(); } catch {} };
  }, [continuousMode, voiceState, playSound]);

  // â”€â”€ Notification Permission + Reminder Scheduling â”€â”€
  useEffect(() => {
    if (typeof Notification !== "undefined" && Notification.permission === "default") {
      Notification.requestPermission();
    }
  }, []);

  const scheduleReminder = useCallback((message: string, delayMs: number) => {
    const reminderTimeout = setTimeout(() => {
      if (Notification.permission === "granted") {
        new Notification("ğŸ”” MISSI Reminder", { body: message, icon: "/favicon.ico" });
      }
      playSound("success");
    }, delayMs);
    return reminderTimeout;
  }, [playSound]);

  // â”€â”€ Voice Change Handler â”€â”€
  const [currentVoiceId, setCurrentVoiceId] = useState("cjVigY5qzO86Huf0OWal"); // Eric default
  const currentVoiceIdRef = useRef(currentVoiceId);
  useEffect(() => { currentVoiceIdRef.current = currentVoiceId; }, [currentVoiceId]);

  const toolIcons: Record<string, string> = {
    web_search: "ğŸ”", get_weather: "ğŸŒ¤ï¸", get_time: "ğŸ•", calculate: "ğŸ”¢",
    run_code: "ğŸ’»", read_webpage: "ğŸ“„", create_document: "ğŸ“", translate: "ğŸŒ",
    analyze_data: "ğŸ“Š", generate_code: "âŒ¨ï¸", set_reminder: "â°", summarize_text: "ğŸ“‹",
    search_gmail: "ğŸ“§", read_gmail: "ğŸ“§", search_files: "ğŸ“‚", get_calendar: "ğŸ“…",
    get_stock_price: "ğŸ“ˆ", get_crypto_price: "ğŸª™", wikipedia: "ğŸ“–", get_location: "ğŸ“", change_voice: "ğŸ­",
  };

  const modelColors: Record<string, string> = {
    "mistral-small-latest": "text-emerald-400",
    "mistral-large-latest": "text-violet-400",
    "codestral-latest": "text-blue-400",
    "pixtral-large-latest": "text-pink-400",
  };

  const modelIcons: Record<string, string> = {
    "mistral-small-latest": "âš¡",
    "mistral-large-latest": "ğŸ§ ",
    "codestral-latest": "ğŸ’»",
    "pixtral-large-latest": "ğŸ‘ï¸",
  };

  // Memoized stats
  const stats = useMemo(() => {
    const totalTools = messages.reduce((s, m) => s + (m.toolCalls?.length || 0), 0);
    const models = new Set(messages.filter(m => m.model).map(m => m.model!.model));
    return { totalTools, uniqueModels: models.size };
  }, [messages]);

  return (
    <div
      className="h-screen bg-[#030508] text-white flex flex-col relative overflow-hidden select-none"
      onDragOver={(e) => { e.preventDefault(); setDragOver(true); }}
      onDragLeave={() => setDragOver(false)}
      onDrop={(e) => { e.preventDefault(); setDragOver(false); const f = e.dataTransfer.files[0]; if (f) handleImageUpload(f); }}
    >
      {/* BG */}
      <div className="absolute inset-0 pointer-events-none">
        <div className="absolute inset-0 bg-[radial-gradient(ellipse_60%_50%_at_50%_40%,rgba(6,182,212,0.03)_0%,transparent_60%)]" />
        <div className="absolute inset-0 opacity-[0.015]" style={{
          backgroundImage: "linear-gradient(rgba(255,255,255,.08) 1px, transparent 1px), linear-gradient(90deg, rgba(255,255,255,.08) 1px, transparent 1px)",
          backgroundSize: "80px 80px",
        }} />
      </div>

      {/* Drop zone */}
      {dragOver && (
        <div className="absolute inset-4 z-50 bg-cyan-500/5 backdrop-blur-md flex items-center justify-center border border-dashed border-cyan-500/30 rounded-3xl">
          <div className="text-center">
            <p className="text-4xl mb-2">ğŸ‘ï¸</p>
            <p className="text-cyan-300 text-sm font-mono">Drop image for Pixtral analysis</p>
          </div>
        </div>
      )}

      {/* Header */}
      <header className="relative z-10 px-4 sm:px-6 py-3 flex items-center justify-between border-b border-white/[0.03]">
        <div className="flex items-center gap-2.5">
          <div className="w-7 h-7 rounded-md bg-gradient-to-br from-cyan-400 to-blue-600 flex items-center justify-center text-[10px] font-black tracking-tighter">M</div>
          <div className="hidden sm:block">
            <h1 className="text-xs font-semibold tracking-widest text-zinc-300">MISSI</h1>
            <p className="text-[8px] text-zinc-600 font-mono tracking-[0.15em]">VOICE AI Â· POWERED BY MISTRAL</p>
          </div>
          {currentModel && (
            <span className={`hidden sm:inline-flex items-center gap-1 ml-2 text-[9px] font-mono px-2 py-0.5 rounded-full bg-white/[0.03] border border-white/[0.04] ${modelColors[currentModel.model] || "text-zinc-400"}`}>
              {modelIcons[currentModel.model]} {currentModel.label}
            </span>
          )}
        </div>
        <div className="flex items-center gap-1.5">
          {stats.totalTools > 0 && (
            <span className="hidden sm:inline text-[9px] font-mono text-zinc-600 px-2 py-0.5 rounded-full bg-white/[0.02]">
              {stats.totalTools} tool{stats.totalTools !== 1 ? "s" : ""} Â· {stats.uniqueModels} model{stats.uniqueModels !== 1 ? "s" : ""}
            </span>
          )}
          <select value={sttLang} onChange={(e) => setSttLang(e.target.value)}
            className="text-[9px] font-mono px-2 py-1 rounded-full border border-white/[0.05] text-zinc-500 bg-transparent hover:text-zinc-300 transition-all cursor-pointer outline-none appearance-none">
            <option value="en-US">ğŸ‡ºğŸ‡¸ EN</option>
            <option value="de-DE">ğŸ‡©ğŸ‡ª DE</option>
            <option value="fr-FR">ğŸ‡«ğŸ‡· FR</option>
            <option value="es-ES">ğŸ‡ªğŸ‡¸ ES</option>
            <option value="ja-JP">ğŸ‡¯ğŸ‡µ JP</option>
            <option value="zh-CN">ğŸ‡¨ğŸ‡³ ZH</option>
            <option value="ko-KR">ğŸ‡°ğŸ‡· KR</option>
            <option value="pt-BR">ğŸ‡§ğŸ‡· PT</option>
            <option value="it-IT">ğŸ‡®ğŸ‡¹ IT</option>
            <option value="ar-SA">ğŸ‡¸ğŸ‡¦ AR</option>
          </select>
          {/* Permission buttons */}
          <button onClick={connectFolder}
            className={`text-[9px] font-mono px-2 py-1 rounded-full border transition-all ${
              permissions.folderFiles ? "border-emerald-500/30 text-emerald-400 bg-emerald-500/5" : "border-white/[0.05] text-zinc-600 hover:text-zinc-400"
            }`} title={permissions.folderFiles ? `${permissions.folderFiles.length} files indexed` : "Grant folder access"}>
            ğŸ“‚ {permissions.folderFiles ? `${permissions.folderFiles.length}` : "Files"}
          </button>
          <button onClick={connectGmail}
            className={`text-[9px] font-mono px-2 py-1 rounded-full border transition-all ${
              permissions.gmailToken ? "border-emerald-500/30 text-emerald-400 bg-emerald-500/5" : "border-white/[0.05] text-zinc-600 hover:text-zinc-400"
            }`} title={permissions.gmailToken ? "Gmail connected" : "Connect Gmail (read-only)"}>
            ğŸ“§ {permissions.gmailToken ? "Gmail âœ“" : "Gmail"}
          </button>
          <button onClick={continuousMode ? deactivate : activate}
            className={`text-[9px] font-mono px-2.5 py-1 rounded-full border transition-all ${
              continuousMode ? "border-cyan-500/30 text-cyan-400 bg-cyan-500/5" : "border-white/[0.05] text-zinc-600 hover:text-zinc-400"
            }`}>
            {continuousMode ? "â— ACTIVE" : "â—‹ VOICE"}
          </button>
          <button onClick={() => setShowChat(!showChat)}
            className={`text-[9px] font-mono px-2.5 py-1 rounded-full border transition-all ${
              showChat ? "border-white/[0.08] text-zinc-400" : "border-white/[0.05] text-zinc-600 hover:text-zinc-400"
            }`}>
            {showChat ? "â—‰" : "â—‹"} CHAT
          </button>
          {messages.length > 0 && (
            <button onClick={clearConversation}
              className="text-[9px] font-mono px-2.5 py-1 rounded-full border border-white/[0.05] text-zinc-600 hover:text-red-400 hover:border-red-500/20 transition-all">
              âœ•
            </button>
          )}
        </div>
      </header>

      {/* Main */}
      <div className="relative z-10 flex-1 flex overflow-hidden">
        {/* Orb View */}
        <div className="flex-1 flex flex-col items-center justify-center px-4">
          {/* Plan */}
          {currentPlan && isLoading && (
            <div className="mb-4 w-full max-w-[260px] animate-in fade-in slide-in-from-bottom-2 duration-300">
              <p className="text-[8px] font-mono text-amber-500/40 uppercase tracking-[0.2em] mb-1.5">Execution Plan</p>
              {currentPlan.map((step, i) => (
                <div key={i} className="flex items-start gap-1.5 py-0.5 text-[11px] text-amber-400/60">
                  <span className="font-mono text-[9px] mt-0.5 shrink-0 w-3">âœ“</span>
                  <span className="leading-snug">{step}</span>
                </div>
              ))}
            </div>
          )}

          <VoiceOrb state={voiceState} audioLevel={audioLevel} size={240} onClick={handleOrbClick} />

          {/* Thinking status â€” live tool progress below orb */}
          {isLoading && thinkingStatus && (
            <p className="mt-3 text-amber-400/60 text-xs font-mono text-center animate-pulse">
              {thinkingStatus}
            </p>
          )}
          {isLoading && !thinkingStatus && (
            <p className="mt-3 text-zinc-600 text-xs font-mono text-center animate-pulse tracking-wider">
              PROCESSINGâ€¦
            </p>
          )}

          {/* Transcript */}
          {input && voiceState === "listening" && (
            <p className="mt-4 text-zinc-400 text-sm italic max-w-md text-center animate-in fade-in duration-200">&quot;{input}&quot;</p>
          )}

          {/* Response (orb view, with typing) */}
          {!showChat && latestContent && !isLoading && (
            <div className="mt-5 max-w-lg text-center px-4 animate-in fade-in slide-in-from-bottom-3 duration-500">
              <p className="text-zinc-300 text-sm leading-relaxed">
                {voiceState === "speaking" ? spokenSoFar : latestContent}
                {voiceState === "speaking" && <span className="inline-block w-[2px] h-[14px] bg-cyan-400 ml-0.5 animate-pulse align-text-bottom" />}
              </p>
              {messages.length > 0 && messages[messages.length - 1].toolCalls && (
                <div className="mt-2.5 flex flex-wrap gap-1 justify-center">
                  {messages[messages.length - 1].toolCalls!.map((t, i) => (
                    <span key={i} className="text-[9px] font-mono text-cyan-500/50 bg-cyan-500/5 px-1.5 py-0.5 rounded-full border border-cyan-500/10">
                      {toolIcons[t.tool]} {t.tool}
                    </span>
                  ))}
                </div>
              )}
            </div>
          )}

          {/* Quick actions */}
          {messages.length === 0 && voiceState === "idle" && !isLoading && (
            <div className="mt-6 max-w-lg px-4 animate-in fade-in duration-700 delay-300">
              <div className="flex flex-wrap gap-1.5 justify-center">
                {(() => {
                  const promptsByLang: Record<string, string[]> = {
                    "de": [
                      "Recherchiere die neuesten KI-DurchbrÃ¼che und erstelle einen Bericht",
                      "Wie ist das Wetter in Berlin?",
                      "Schreibe eine Python Quicksort-Funktion",
                      "Ãœbersetze 'Hallo' in 5 Sprachen",
                    ],
                    "fr": [
                      "Recherche les derniÃ¨res avancÃ©es en IA et crÃ©e un rapport",
                      "Quel temps fait-il Ã  Paris?",
                      "Ã‰cris une fonction quicksort en Python",
                      "Traduis 'bonjour' en 5 langues",
                    ],
                    "es": [
                      "Investiga los Ãºltimos avances en IA y crea un informe",
                      "Â¿QuÃ© tiempo hace en Madrid?",
                      "Escribe una funciÃ³n quicksort en Python",
                      "Traduce 'hola' a 5 idiomas",
                    ],
                    "en": [
                      "Research latest AI breakthroughs and create a report",
                      "What's the weather in Tokyo?",
                      "Write a Python quicksort function",
                      "Translate 'hello' into 5 languages",
                    ],
                  };
                  const langKey = sttLang.split("-")[0];
                  const prompts = promptsByLang[langKey] || promptsByLang["en"];
                  return prompts.map((q) => (
                    <button key={q} onClick={() => sendMessage(q)}
                      className="text-[11px] text-zinc-400 hover:text-white bg-white/[0.03] hover:bg-white/[0.07] border border-white/[0.06] hover:border-cyan-500/20 px-3.5 py-2 rounded-xl transition-all duration-200 hover:shadow-[0_0_12px_rgba(6,182,212,0.08)]">
                      {q}
                    </button>
                  ));
                })()}
              </div>
              <p className="text-[9px] text-zinc-700 font-mono text-center mt-3 tracking-wide">
                Space = Talk Â· Esc = Stop Â· âŒ˜K = Clear
              </p>
            </div>
          )}

          {/* Input */}
          <div className="mt-4 w-full max-w-md px-4">
            <div className="flex items-center gap-1.5 bg-white/[0.03] border border-white/[0.06] rounded-2xl px-3 py-1 focus-within:border-cyan-500/25 focus-within:shadow-[0_0_20px_rgba(6,182,212,0.06)] transition-all duration-300">
              <button onClick={() => fileInputRef.current?.click()}
                className="w-7 h-7 rounded-lg flex items-center justify-center text-zinc-700 hover:text-cyan-400 hover:bg-white/[0.03] transition-all" title="Upload image (Pixtral)">
                <svg width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round"><path d="M21.44 11.05l-9.19 9.19a6 6 0 01-8.49-8.49l9.19-9.19a4 4 0 015.66 5.66l-9.2 9.19a2 2 0 01-2.83-2.83l8.49-8.48"/></svg>
              </button>
              <input type="file" ref={fileInputRef} accept="image/*" className="hidden"
                onChange={(e) => { const f = e.target.files?.[0]; if (f) handleImageUpload(f); }} />
              <input ref={inputRef} type="text" value={input} onChange={(e) => setInput(e.target.value)} onKeyDown={handleKeyDown}
                placeholder="Ask MISSI anything..."
                className="flex-1 bg-transparent py-2.5 text-sm outline-none placeholder:text-zinc-600 min-w-0" />
              <button onClick={() => sendMessage(input)} disabled={isLoading || !input.trim()}
                className="w-7 h-7 rounded-lg flex items-center justify-center bg-cyan-600 hover:bg-cyan-500 disabled:opacity-15 text-white text-xs font-bold transition-all shrink-0">
                <svg width="12" height="12" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="3" strokeLinecap="round" strokeLinejoin="round"><path d="M12 19V5M5 12l7-7 7 7"/></svg>
              </button>
            </div>
          </div>
        </div>

        {/* Chat Panel */}
        <div className={`border-l border-white/[0.03] flex flex-col bg-[#050810]/90 backdrop-blur-2xl transition-all duration-300 ${
          showChat ? "w-[380px] sm:w-[440px] opacity-100" : "w-0 opacity-0 overflow-hidden"
        }`}>
          <div className="px-3 py-2.5 border-b border-white/[0.03] flex items-center justify-between shrink-0">
            <span className="text-[9px] font-mono text-zinc-500 tracking-wider">ğŸ’¬ {messages.length} {messages.length === 1 ? "MESSAGE" : "MESSAGES"}</span>
          </div>

          <div className="flex-1 overflow-y-auto p-3 space-y-2.5 min-h-0">
            {messages.length === 0 && !isLoading && (
              <div className="flex flex-col items-center justify-center h-full text-center opacity-40">
                <div className="text-3xl mb-3">ğŸ™ï¸</div>
                <p className="text-zinc-500 text-xs font-mono">Ready to assist</p>
                <p className="text-zinc-700 text-[9px] mt-1">Say &quot;Hey Missi&quot; or tap the orb</p>
              </div>
            )}
            {messages.map((msg, i) => (
              <div key={i} className="space-y-1 animate-in fade-in slide-in-from-bottom-1 duration-200">
                {/* Model tag */}
                {msg.role === "assistant" && msg.model && (
                  <div className={`flex items-center gap-1 text-[8px] font-mono ${modelColors[msg.model.model] || "text-zinc-600"}`}>
                    {modelIcons[msg.model.model]} {msg.model.label}
                  </div>
                )}

                {/* Plan */}
                {msg.plan && (
                  <div className="p-2 bg-amber-500/[0.03] border border-amber-500/10 rounded-xl text-[10px]">
                    <p className="text-amber-500/40 font-mono uppercase text-[8px] tracking-wider mb-1">Plan</p>
                    {msg.plan.map((s, j) => (
                      <div key={j} className="flex items-start gap-1 text-amber-400/50 py-px">
                        <span className="text-[8px] mt-px">âœ“</span><span>{s}</span>
                      </div>
                    ))}
                  </div>
                )}

                {/* Image */}
                {msg.image && (
                  <div className="flex justify-end">
                    {/* eslint-disable-next-line @next/next/no-img-element */}
                    <img src={msg.image} alt="" className="max-w-[180px] rounded-xl border border-white/[0.06] shadow-lg" />
                  </div>
                )}

                {/* Bubble */}
                <div className={`flex ${msg.role === "user" ? "justify-end" : "justify-start"}`}>
                  <div className={`max-w-[88%] rounded-2xl px-3 py-2 ${
                    msg.role === "user"
                      ? "bg-cyan-500/8 text-cyan-50 border border-cyan-500/10"
                      : "bg-white/[0.02] text-zinc-300 border border-white/[0.03]"
                  }`}>
                    {/* Voice input indicator */}
                    {msg.role === "user" && msg.fromVoice && (
                      <span className="text-[9px] text-cyan-500/40 font-mono">ğŸ™ï¸ voice</span>
                    )}
                    <p className="whitespace-pre-wrap text-[12.5px] leading-[1.6]">
                      {msg.role === "assistant" && msg.displayedContent !== undefined && msg.displayedContent !== msg.content && msg.displayedContent !== ""
                        ? <>{msg.displayedContent}<span className="inline-block w-[2px] h-[14px] bg-cyan-400 ml-0.5 animate-pulse align-text-bottom" /></>
                        : msg.content
                      }
                    </p>
                    {/* Response time + tools count */}
                    {msg.role === "assistant" && msg.responseTime && (
                      <div className="mt-1.5 flex items-center gap-1.5 text-[8px] font-mono text-zinc-600">
                        <span className="bg-white/[0.03] px-1.5 py-0.5 rounded">âš¡ {(msg.responseTime / 1000).toFixed(1)}s</span>
                        {msg.toolCalls && msg.toolCalls.length > 0 && <span className="bg-white/[0.03] px-1.5 py-0.5 rounded">ğŸ›  {msg.toolCalls.length} tool{msg.toolCalls.length !== 1 ? "s" : ""}</span>}
                        {msg.model && <span className="bg-white/[0.03] px-1.5 py-0.5 rounded">{modelIcons[msg.model.model]} {msg.model.label}</span>}
                      </div>
                    )}
                  </div>
                </div>

                {/* Tools */}
                {msg.toolCalls && msg.toolCalls.length > 0 && (
                  <div className="pl-1 space-y-0.5">
                    {msg.toolCalls.map((t, j) => (
                      <details key={j} className="group">
                        <summary className="flex items-center gap-1 cursor-pointer text-[9px] font-mono text-zinc-600 hover:text-zinc-400">
                          <span className="text-[10px]">{toolIcons[t.tool]}</span>
                          <span>{t.tool}</span>
                          <span className="text-zinc-800 truncate max-w-[120px]">
                            {t.args && Object.values(t.args)[0] ? `(${Object.values(t.args)[0]})` : ""}
                          </span>
                          <span className="text-zinc-800 ml-auto">{t.duration}ms</span>
                          <span className="text-[7px] text-zinc-700 group-open:rotate-90 transition-transform">â–¶</span>
                        </summary>
                        <pre className="mt-0.5 text-[9px] text-zinc-700 bg-black/40 rounded-lg p-1.5 overflow-x-auto border border-white/[0.02] max-h-24 overflow-y-auto leading-relaxed">{t.result}</pre>
                      </details>
                    ))}
                  </div>
                )}

                {/* Documents */}
                {msg.documents?.map((doc, j) => (
                  <button key={j} onClick={() => downloadDocument(doc)}
                    className="flex items-center gap-2 w-full text-left p-2 bg-emerald-500/[0.03] border border-emerald-500/10 rounded-xl hover:bg-emerald-500/[0.06] transition-all group">
                    <div className="w-7 h-7 rounded-lg bg-emerald-500/10 flex items-center justify-center text-xs">ğŸ“„</div>
                    <div className="flex-1 min-w-0">
                      <p className="text-[11px] font-medium text-emerald-300 truncate">{doc.title}</p>
                      <p className="text-[9px] text-zinc-600">{doc.type} Â· download</p>
                    </div>
                    <span className="text-emerald-500/30 group-hover:text-emerald-400 text-sm">â†“</span>
                  </button>
                ))}
              </div>
            ))}

            {/* Live Tool Execution Cards */}
            {isLoading && activeTools.length > 0 && (
              <div className="space-y-1 animate-in fade-in duration-200">
                {activeTools.map((t, j) => (
                  <div key={j} className={`flex items-start gap-2 px-3 py-2 rounded-xl border transition-all duration-300 ${
                    t.status === "running"
                      ? "bg-amber-500/[0.04] border-amber-500/15"
                      : "bg-emerald-500/[0.03] border-emerald-500/10"
                  }`}>
                    <span className="text-sm mt-0.5">{
                      t.status === "running"
                        ? <span className="inline-block animate-spin">â³</span>
                        : "âœ…"
                    }</span>
                    <div className="flex-1 min-w-0">
                      <div className="flex items-center gap-1.5">
                        <span className="text-[10px] font-mono font-medium text-zinc-300">{toolIcons[t.tool]} {t.tool}</span>
                        {t.duration && <span className="text-[8px] font-mono text-zinc-700">{t.duration}ms</span>}
                      </div>
                      {t.args && Object.values(t.args)[0] && (
                        <p className="text-[9px] text-zinc-500 truncate mt-0.5">
                          {String(Object.values(t.args)[0]).slice(0, 60)}
                        </p>
                      )}
                      {t.status === "done" && t.result && (
                        <p className="text-[9px] text-zinc-600 mt-0.5 line-clamp-2 leading-relaxed">
                          {t.result.slice(0, 120)}...
                        </p>
                      )}
                    </div>
                  </div>
                ))}
              </div>
            )}

            {isLoading && (
              <div className="flex justify-start animate-in fade-in duration-200">
                <div className="bg-white/[0.02] rounded-2xl px-3 py-2.5 border border-white/[0.03]">
                  <div className="flex items-center gap-2">
                    <div className="flex gap-1">
                      <div className="w-1.5 h-1.5 bg-cyan-500 rounded-full animate-bounce" />
                      <div className="w-1.5 h-1.5 bg-cyan-500 rounded-full animate-bounce [animation-delay:0.15s]" />
                      <div className="w-1.5 h-1.5 bg-cyan-500 rounded-full animate-bounce [animation-delay:0.3s]" />
                    </div>
                    {currentModel && <span className={`text-[8px] font-mono ${modelColors[currentModel.model]}`}>{modelIcons[currentModel.model]} {currentModel.label}</span>}
                  </div>
                  {currentPlan && (
                    <div className="mt-2 pt-2 border-t border-white/[0.03]">
                      {currentPlan.map((step, j) => (
                        <div key={j} className="flex items-start gap-1 text-[9px] text-amber-400/50 py-0.5">
                          <span className="text-[8px] mt-px animate-pulse">â—</span>
                          <span>{step}</span>
                        </div>
                      ))}
                    </div>
                  )}
                </div>
              </div>
            )}
            <div ref={messagesEndRef} />
          </div>
        </div>
      </div>

      {/* Footer */}
      <footer className="relative z-10 py-2 text-center border-t border-white/[0.03]">
        <p className="text-[8px] text-zinc-600 font-mono tracking-[0.2em]">
          21 TOOLS Â· 4 MISTRAL MODELS Â· ELEVENLABS TTS Â· 10 LANGUAGES Â· MIMI TECH AI
        </p>
      </footer>

      <audio ref={audioRef} crossOrigin="anonymous" />
    </div>
  );
}
